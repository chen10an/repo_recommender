{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset for GitHub Repository Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** make a dataset (pandas dataframe) of repo ratings 1/0 (star/no star)  \n",
    "**Dataset Dimensions:** no. repos * no. users\n",
    "\n",
    "1. get the names of the top 1000 repos (sorted by no. stars) --> list\n",
    "2. get the names and starred repos of users with 30-50 followers --> dict of username:list of starred repos\n",
    "3. make dataframe (repos*users) \n",
    "  * initialize with all zeros\n",
    "  * row indices (i): names of the top 1000 repos\n",
    "  * col indices (j): names of all users with 30-50 followers\n",
    "  * if user j has starred repo i: df.loc[i,j]=1\n",
    "4. make csv file out of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "# libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# python file with authentication details\n",
    "import auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url base\n",
    "github = 'https://api.github.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_note: after this point, it is possible to start executing code from any \"# initialization\" cell (assuming the necessary pickle files are present)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. get the names of the top 1000 repos (sorted by no. stars) --> list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "params = {\n",
    "    'per_page': 100,\n",
    "    'page': 1,\n",
    "    'q': 'stars:>6000',  # 1,195 repos as of Aug. 8 2017\n",
    "    'sort': 'stars'\n",
    "}\n",
    "\n",
    "lastPage = 10\n",
    "repoList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get 10*100=1000 top repos sorted by no. stars\n",
    "# and place these repo names into repoList\n",
    "for i in range(lastPage):\n",
    "    print(\"getting page {}\".format(params['page']))\n",
    "    r = requests.get(github + '/search/repositories', auth=(auth.user, auth.pw), params=params)\n",
    "    for repo in r.json()['items']:\n",
    "        name = repo['full_name']\n",
    "        repoList.append(name)\n",
    "    params['page'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(repoList)  # should be 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save repoList\n",
    "pickle.dump(repoList, open('repoList.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. get the names and starred repos of users with 30-50 followers --> dict of username:list of starred repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 get the names of all users with 30-50 followers --> list of usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "params = {\n",
    "    'per_page': 100,\n",
    "    'page': 1,\n",
    "    'q': 'followers:30',\n",
    "    'sort': 'repositories'\n",
    "}\n",
    "\n",
    "userList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing time to process each request (interrupt kernel after a few examples)\n",
    "# same code below without time.time()\n",
    "\n",
    "for i in range(20):\n",
    "    n = 30+(i)\n",
    "    q = 'followers:' + str(n)\n",
    "    print(\"query: {}\".format(q))\n",
    "    params['q'] = q\n",
    "    \n",
    "    lastPage = 1\n",
    "    params['page'] = 1\n",
    "    \n",
    "    while params['page'] <= lastPage:\n",
    "        start = time.time()\n",
    "        r = requests.get(github + '/search/users', auth=(auth.user, auth.pw), params=params)\n",
    "\n",
    "        if params['page'] == 1:\n",
    "            s = r.headers['Link'].split(', ')[1]\n",
    "            match = re.search(r'&page=([0-9]+)', s)\n",
    "            lastPage = int(match.group(1))\n",
    "\n",
    "        for user in r.json()['items']:\n",
    "            name = user['login']\n",
    "            userList.append(name)\n",
    "            \n",
    "        params['page'] += 1\n",
    "        print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "params = {\n",
    "    'per_page': 100,\n",
    "    'page': 1,\n",
    "    'q': 'followers:30',\n",
    "    'sort': 'repositories'\n",
    "}\n",
    "\n",
    "userList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get users with 30 <= no. followers < 50\n",
    "# and place these usernames into userDict, i.e. userDict[name]=[]\n",
    "\n",
    "# the search api has a rate limit of 30 requests per minute, or 1 request per 2s\n",
    "# so adding time.sleep(1.5) to the time to process each request (~0.7s) \n",
    "# will make sure that the rate limit is not exceeded\n",
    "\n",
    "# the search api returns up to 1,000 results for each search\n",
    "# so 'q': 'followers:30..50' will return only 1000 users \n",
    "# when there are 36,254 users that fit this query (as of Aug. 9 2017)\n",
    "# instead of 'followers:30..50', I will use 'followers:30', 'followers:31', ..., 'followers:49'\n",
    "# to get the top 1000 users (or all users if no. users < 1000) for each of these queries \n",
    "# (users are sorted by no. repos)\n",
    "\n",
    "for i in range(20):\n",
    "    # change query\n",
    "    n = 30+(i)\n",
    "    q = 'followers:' + str(n)\n",
    "    print(\"query: {}\".format(q))\n",
    "    params['q'] = q\n",
    "    \n",
    "    # reset params for while loop\n",
    "    lastPage = 1\n",
    "    params['page'] = 1\n",
    "    \n",
    "    while params['page'] <= lastPage:\n",
    "        print(\"getting page {}\".format(params['page']))\n",
    "        r = requests.get(github + '/search/users', auth=(auth.user, auth.pw), params=params)\n",
    "\n",
    "        if params['page'] == 1:\n",
    "            s = r.headers['Link'].split(', ')[1]  # info for last page\n",
    "            match = re.search(r'&page=([0-9]+)', s)\n",
    "            lastPage = int(match.group(1))\n",
    "\n",
    "        for user in r.json()['items']:\n",
    "            name = user['login']\n",
    "            userList.append(name)\n",
    "            \n",
    "        params['page'] += 1\n",
    "        time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(userList)  # should be 20,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save userList\n",
    "pickle.dump(userList, open('userList.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 get the 100 most recently starred repos for all users in userList --> dict of username:list of starred repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "params = {\n",
    "    'per_page': 100,\n",
    "    'page': 1\n",
    "}\n",
    "\n",
    "userList = pickle.load(open('userList.pickle', 'rb'))\n",
    "userDict = {u:[] for u in userList}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing time to process each request (interrupt kernel after a few examples)\n",
    "# same code below without time.time()\n",
    "\n",
    "for username in userList:\n",
    "    start = time.time()\n",
    "    r = requests.get(github + '/users/' + username + '/starred', auth=(auth.user, auth.pw), params=params)\n",
    "    \n",
    "    if r.status_code == requests.codes.ok:\n",
    "        for repo in r.json():\n",
    "            name = repo['full_name']\n",
    "            userDict[username].append(name)\n",
    "\n",
    "    print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "params = {\n",
    "    'per_page': 100,\n",
    "    'page': 1\n",
    "}\n",
    "\n",
    "userList = pickle.load(open('userList.pickle', 'rb'))\n",
    "userDict = {u:[] for u in userList}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get each user's 100 most recently starred repos (or all repos if no. repos < 100)\n",
    "# and place these repo names into userDict, i.e. userDict[username]=[repo0, repo1, ..., repo100]\n",
    "\n",
    "# the standard rate limit is 5000 requests per hour, or 1.4 requests per second --> 1 request per 0.7s\n",
    "# the average time needed to process each request seems to well exceed 0.7s so no time.sleep() is needed\n",
    "# this loop takes ~6hrs\n",
    "\n",
    "for username in tqdm(userList):\n",
    "    r = requests.get(github + '/users/' + username + '/starred', auth=(auth.user, auth.pw), params=params)\n",
    "    \n",
    "    # important: check for successful get request in case e.g. someone deletes their account\n",
    "    # (this error happened on my first try to make userDict: 11756/20000 requests and 4hrs wait...)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        for repo in r.json():\n",
    "            name = repo['full_name']\n",
    "            userDict[username].append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(userDict)  # should be 20,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save userDict\n",
    "pickle.dump(userDict, open('userDict.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make dataframe (repos*users)\n",
    "* initialize with all zeros\n",
    "* row indices (i): names of the top 1000 repos\n",
    "* col indices (j): names of all users with 30-50 followers\n",
    "* if user j has starred repo i: df.loc[i,j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "repoList = pickle.load(open('repoList.pickle', 'rb'))\n",
    "userList = pickle.load(open('userList.pickle', 'rb'))\n",
    "userDict = pickle.load(open('userDict.pickle', 'rb'))\n",
    "\n",
    "Y_df =  pd.DataFrame(0, index=repoList, columns=userList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_df.shape  # should be (1000, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if user j has starred repo i: df.loc[i,j]=1\n",
    "\n",
    "for j,starList in tqdm(userDict.items()):\n",
    "    for i in starList:\n",
    "        if i in repoList:\n",
    "            Y_df.loc[i,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(Y_df.values)  # number of 1s in df should be 170,543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check for empty rows, i.e. repos with no stars\n",
    "for row in Y_df.iterrows():\n",
    "    if np.sum(row[1].values) == 0:\n",
    "        print(row[0])  # should be nothing printed, no empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check for empty columns, i.e. users that have not starred any of the top 1000 repos\n",
    "emptyColNames = []\n",
    "for col in Y_df.iteritems():\n",
    "    if np.sum(col[1].values) == 0:\n",
    "        emptyColNames.append(col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(emptyColNames)  # should be 2953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop empty columns from final df\n",
    "Y_df.drop(emptyColNames, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_df.shape  # should be (1000, 17047)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity: (no. stars where df.loc[i,j]=1)/(no. possible stars)\n",
    "np.sum(Y_df.values)/np.size(Y_df.values)  # should be ~1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save Y_df\n",
    "pickle.dump(Y_df, open('Y_df.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. make csv file out of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "Y_df = pickle.load(open('Y_df.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make csv file from Y_df\n",
    "Y_df.to_csv('Y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test that the csv file works\n",
    "temp = pd.read_csv('Y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp.head() # row names should be in the first col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp.shape  # should be (1000, 17047+1) because of the extra row name col"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
